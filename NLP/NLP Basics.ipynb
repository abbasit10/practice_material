{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe440cce",
   "metadata": {},
   "source": [
    "# Natural Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9a1dc",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "It may be defined as the process of breaking up a piece of text into smaller parts, such as sentences and words. These smaller parts are called tokens. For example, a word is a token in a sentence, and a sentence is a token in a paragraph.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5cef4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370141e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'you', 'go', '.', 'Is', 'this', 'what', 'you', 'are', 'looking', 'for', '?']\n"
     ]
    }
   ],
   "source": [
    "text= 'Here you go. Is this what you are looking for?'\n",
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e7cb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wo', \"n't\"]\n"
     ]
    }
   ],
   "source": [
    "text_0= '''Won't'''\n",
    "print(word_tokenize(text_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00f19a",
   "metadata": {},
   "source": [
    "* Here in second example the word_tokenize splitts the one word i.e 'won't' into 'Wo' and 'n't'. In order to avoid it, we can use alternate tokenizer i.e WordPunctTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96cc415b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'can', \"'\", 't', 'allow', 'you', 'to', 'go', 'home', 'earlier', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPunctTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text= '''I can't allow you to go home earlier.'''\n",
    "\n",
    "tokenizer= WordPunctTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8232d",
   "metadata": {},
   "source": [
    "## Tokenizing text into sentences.\n",
    "It is used to tokenize the sentences from the paragraph. For this purpose we have sent_tokenize in nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28c46cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!',\n",
       " 'is anyone here?',\n",
       " 'I need to talk to someone.',\n",
       " 'I am looking for a room here.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text= '''Hello! is anyone here? I need to talk to someone. I am looking for a room here.'''\n",
    "\n",
    "sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b79a2",
   "metadata": {},
   "source": [
    "## RageexTokenizer\n",
    "Unlike standard tokenizers that split text on predefined characters like spaces and punctuation, RegexpTokenizer allows you to define your own rules for tokenization using regular expressions. This is particularly useful when the standard tokenization doesn't suit your needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb35844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'is',\n",
       " 'anyone',\n",
       " 'here',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'need',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'someone']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text= '''Hello! is anyone here? I need to talk to someone. I am looking for a room here.'''\n",
    "\n",
    "tokenizer= RegexpTokenizer(\"[\\w']+\")\n",
    "text= \"Hello! is anyone here? I don't need to talk to someone.\"\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560ddd8",
   "metadata": {},
   "source": [
    "Here \"don't\" is consdidered as one token instead of tokenizing into 2. And Punctuation is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2425ca",
   "metadata": {},
   "source": [
    "## Extraction of text from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be93eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366e87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= webtext.raw(r\"X:\\BIA\\nlpt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7949cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer= PunktSentenceTokenizer(text)\n",
    "sents_1= sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fcd6311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Samsung AI Ballie\\nSamsung is going to launch AI Robot named Ballie, it can also be called as AI home assistant.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8d15ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It can do some interesting things inside our homes.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "704c3f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the methods and backgrounds behind this advancement?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_1[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a143b",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "Some common words that are present in text but do not contribute in the meaning of a\n",
    "sentence. Such words are not at all important for the purpose of information retrieval or\n",
    "natural language processing. The most common stopwords are ‘the’ and ‘a’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4ddd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing stopwords from nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "# stopwords\n",
    "stop_words= stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b20570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Am',\n",
       " 'I',\n",
       " 'doing',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'right',\n",
       " 'way',\n",
       " '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= ' Hello how are you? Am I doing it in a right way?'\n",
    "text= word_tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85706ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '?', 'Am', 'I', 'right', 'way', '?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= [word for word in text if word not in stop_words]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db4b42",
   "metadata": {},
   "source": [
    "## List of supported languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c853a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710019b5",
   "metadata": {},
   "source": [
    "## Wordnet\n",
    "Following are some use cases of Wordnet:\n",
    "* It can be used to look up the definition of a word\n",
    "* We can find synonyms and antonyms of a word\n",
    "* Word relations and similarities can be explored using Wordnet\n",
    "* Word sense disambiguation for those words having multiple uses and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68103560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc8c736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog.n.01'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn= wn.synsets('dog')[0]\n",
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e11f675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f7013a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lion.n.01'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn2= wn.synsets('lion')[0]\n",
    "syn2.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22d08202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large gregarious predatory feline of Africa and India having a tawny coat with a shaggy mane in the male'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn2.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "470d3561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hammer.n.07'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn= wn.synsets('hammer')[6]\n",
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47036032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a power tool for drilling rocks'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f16948",
   "metadata": {},
   "source": [
    "## What is Stemming?\n",
    "Stemming is a technique used to extract the base form of the words by removing affixes\n",
    "from them. It is just like cutting down the branches of a tree to its stems. For example,\n",
    "the stem of the words eating, eats, eaten is eat.\n",
    "\n",
    "#### Advantage:\n",
    "Search engines use stemming for indexing the words. That’s why rather than storing all\n",
    "forms of a word, a search engine can store only the stems. In this way, stemming reduces\n",
    "the size of the index and increases retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e365314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9b3f37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= 'writing'\n",
    "stemmer= PorterStemmer()\n",
    "x= stemmer.stem(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644d835",
   "metadata": {},
   "source": [
    "### RegexpStemmer\n",
    "With this stemmer, we can manually enter a prefix or suffix to be removed from the text. \n",
    "###### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29689987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6db9b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entertain'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer= RegexpStemmer('ment')\n",
    "x='entertainment'\n",
    "x= stemmer.stem(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e9730",
   "metadata": {},
   "source": [
    "## What is Lemmatization?\n",
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efd8552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5996482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer= WordNetLemmatizer()\n",
    "x= 'eating'\n",
    "lemmatizer.lemmatize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9341c3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= 'believes'\n",
    "lemmatizer.lemmatize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2898e",
   "metadata": {},
   "source": [
    "### Difference between stemming and Lemmatization:'\n",
    "Stemming removes the endings words, while Lemmatization tells about the root of the word.\n",
    "###### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87e99dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From lemmatization we get belief\n",
      "From stemming we get believ\n"
     ]
    }
   ],
   "source": [
    "x= 'believes'\n",
    "x=lemmatizer.lemmatize(x)\n",
    "print(f'From lemmatization we get {x}')\n",
    "\n",
    "x= 'believes'\n",
    "stemmer= PorterStemmer()\n",
    "x= stemmer.stem(x)\n",
    "print(f'From stemming we get {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6192972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
